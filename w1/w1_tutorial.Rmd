---
title: "R Notebook"
output: html_notebook
---

Question A

```{r}
hkre.data <- read.csv("HKRE.csv")
cor(hkre.data,hkre.data$price)
```
- area has the greatest correlation with price => 0.8276062
- floor has the weakest correlation with price => 0.1299963

Question 2

```{r}
hist(hkre.data$price)
hist(log(hkre.data$price))
qqnorm(hkre.data$price)
qqnorm(log(hkre.data$price))
```

Yes we should perform log transformation on the price

Question C
(a)

I will including everything except id because it is not relevant to the price.

```{r}
hkre.data$view <- factor(hkre.data$view)
fit1 <- lm(log(price) ~. -ID, data = hkre.data)
summary(fit1)
AIC(fit1)
```
Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  6.0328082  0.0620921  97.159  < 2e-16 ***
area         0.0003254  0.0000323  10.074  < 2e-16 ***
beds         0.0323501  0.0179970   1.798  0.07530 .  
baths        0.0528778  0.0247379   2.138  0.03502 *  
cars         0.1773474  0.0345127   5.139 1.39e-06 ***
age         -0.0128701  0.0024863  -5.176 1.19e-06 ***
floor        0.0007459  0.0044716   0.167  0.86785    
view1        0.0708676  0.0229778   3.084  0.00265 ** 
view2        0.2046945  0.0396795   5.159 1.28e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1051 on 99 degrees of freedom
Multiple R-squared:  0.8409,	Adjusted R-squared:  0.8281 
F-statistic: 65.43 on 8 and 99 DF,  p-value: < 2.2e-16

Everything but floor and beds are statistically significant.

R sqaure is 0.8281

Question D

```{r}
backward_model_fit1 <- step(fit1, direction = "backward")
summary(backward_model_fit1)
```

Everything but floor are included in this model.

Question D (b)

Log-tranformation of all

```{r}
fit2_full <- lm(log(price) ~ log(area) + log(I(beds+1)) + log(I(baths+1)) + log(I(cars+1)) + log(I(age+1)) + log(I(floor)) + view, data = hkre.data )
backward_fit2 <- step(fit2_full, direction = "backward")
summary(backward_fit2)
```

Square terms for all numeric variables

```{r}
fit3_full <- lm(log(price) ~ I(area^2) + I(beds^2) + I(baths^2) + I(cars^2) + I(age^2) + I(floor^2) + view, data = hkre.data )
backward_fit3 <- stepAIC(fit3_full, direction = "backward")
summary(backward_fit3)
AIC(backward_fit3)
extractAIC(backward_fit3)
```

polynomials model with more than 2 degree

```{r}
fit4_full <- lm(log(price) ~ I(area^3) + I(beds^3) + I(baths^3) + I(cars^3) + I(age^3) + I(floor^3) + view, data = hkre.data )
backward_fit4 <- step(fit4_full, direction = "backward")
summary(backward_fit4)
```

polynomials model with more than 2 degree (second one)

```{r}
fit5_full <- lm(log(price) ~ I(area^4) + I(beds^4) + I(baths^4) + I(cars^4) + I(age^4) + I(floor^4) + view, data = hkre.data )
backward_fit5 <- step(fit5_full, direction = "backward")
summary(backward_fit5)
```


```{r}
fit6_full <- lm(log(price) ~. -ID + area*view + area*cars + area*beds, data = hkre.data )
backward_fit6 <- step(fit6_full, direction = "backward")
summary(backward_fit6)
```



```{r}
# Load the built-in mtcars dataset
data(mtcars)

# Fit the full model with all predictors
full_model <- lm(mpg ~ ., data = mtcars)

# Perform backward selection using step()
selected_model <- step(full_model, direction = "backward")

# Calculate AIC for the full model
aic_full_model <- AIC(full_model)

# Calculate AIC for the final model obtained from step()
aic_selected_model <- AIC(selected_model)

# Fit the intercept-only model
intercept_only_model <- lm(mpg ~ 1, data = mtcars)

# Calculate AIC for the intercept-only model
aic_intercept_only <- AIC(intercept_only_model)

# Print the AIC values
print(paste("AIC of the full model:", aic_full_model))
print(paste("AIC of the final selected model (from step()):", aic_selected_model))
print(paste("AIC of the intercept-only model:", aic_intercept_only))

# Print the final selected model's summary to verify
summary(selected_model)

```

step() function use extractAIC() which calculate the aic value differently to AIC()
but 
extractAIC(full.modell) - extractAIC(null.modell)
and 
AIC(full.modell) - AIC(null.modell)
give the same value, so they are equavalent in practical purposes.

```{r}
extractAIC(fit3_full) - extractAIC(backward_fit3)

AIC(fit3_full) - AIC(backward_fit3)

```

Question E

(b)

```{r}
loop.string <- c("area", "beds", "baths", "cars", "age", "floor", "view")

onevariable= NULL
for(J in 1:length(loop.string)){
  temp.fit=summary(lm(log(hkre.data$price)~hkre.data[,loop.string[J]]))
  onevariable=rbind(onevariable, temp.fit$coefficients[2,])
}
rownames(onevariable)=loop.string
onevariable


```

Question F

(a)

```{r}
z.onevariable = NULL
for(i in 1:length(loop.string)) {
  temp.fit=summary(lm(log(hkre.data$price)~scale(hkre.data[,loop.string[i]])))
  z.onevariable=rbind(z.onevariable, temp.fit$coefficients[2,])
}

rownames(z.onevariable)=loop.string
round(z.onevariable, 2)

#
# standardize the variables
hkre.data_std <- hkre.data %>%
  mutate(across(c(area, beds, baths, cars, age, floor, view), scale))

summary(lm(log(price) ~ area + beds + baths + cars + age + floor + view, data=hkre.data))

full_stan = lm(log(price) ~ area + beds + baths + cars + age + floor + view, data=hkre.data)

barplot(rbind())
```

Question G

2 way to tackle this:

- Does removing the non-significant predictor reduce fit? (not the focus of this class)
- (more relevant to causal) Does removing the non-significant input variables induce omitted variable bias?

```{r}

```

Question H

